\section{Evaluation Plan}
As previously stated, the purpose of this project is not to implement algorithms that are able to provide higher performance than existing software. Rather, it is intended that the packages will provide a set of frameworks that can easily be understood, modified, and expanded, while providing performance not significantly lower than existing optimised packages, and faster than implementations in MATLAB or Python.

This aim shows two key goals; high performing packages, and code that can easily be maintained. The first of these is quantifiable with use of benchmarking on a large range of optimisation problems. This will be a crucial part of testing the packages for stability and and correctness, as well as performance evaluation. A widely used set of problems for this purpose is the CUTEst package of problems. The second aim can be accomplished through application of software development techniques such as stringent documentation and unit-testing. These result in code that can be understood easily, and trusted to work as intended.

\subsection{Benchmarking}\label{sub_evaluation}
CUTEst provides a very large range of problems of a variety of types, with packages making the problems available in most environments, meaning that the same problems can very easily be used to compare this Julia code against its contemporaries. An important factor in the benchmarking of software is ensuring that the environment code is run in is fair between tests. This requires that tests are performed on the same hardware, with steps taken to ensure that the computer's resources are fully available to the tests. 

Ideally tests will also be performed on a variety of different computing platforms to ensure that different hardware doesn't introduce otherwise unknown performance penalties. This also has the potential to identify stability issues on different platforms, for example, some types may not be natively available on some platforms (such as \texttt{Int64} variables on a    32-bit computer). In theory Julia's compilation should be able to adjust for this, but this cannot be proven without testing. To this end, testing on a variety of hardware platforms is crucial.

As well as comparing the packages against implementations in other languages, benchmarking will be very useful when optimising the performance of the packages. As the problems presented in CUTEst are representative of large real problems, they will bring up areas of the code that are causing bottlenecks, and make it easy to compare different approaches, choosing the ones that result in the best overall performance.

\subsection{Documentation}
As the packages developed in this project are intended to continue development in the future, it is crucial that all software is designed in an expandable manner. An important aspect of this is documenting the APIs, features, and requirements of the frameworks to make it clear what the framework supports already, and where it needs to be expanded.

Julia features a system for embedding code documentation within comment blocks, which can be scanned to automatically create formatted documentation. This system removes obstacles that prevent keeping the current documentation up-to-date by placing the documentation right next to the code that is being updated. It should be made a matter of routine that when code is changed in a function, the associated documentation is also edited. This ensures that the code always has an easily available and accurate explanation for its function.

Embedded documentation is very good at providing information on specific parts of the codebase but is poor at giving an overview of package functionality and the intended usage patterns. To this end, a certain amount of documentation discussing the higher level design of the software is needed. This documentation does not need to be as detailed or up-to-date as the embedded documentation, as the concepts they discuss are unlikely to change a great deal except at major releases. These documents will be crucial for ensuring that the package can be properly maintained in future.

The project currently features embedded documentation on over 90\% of its functions, as well as several guides on the use of the package, and developing new algorithms. As the project moves towards a more usable state, these guides will need to be expanded to better explain the internal design in a more cohesive manner.

\subsection{Versioning}
This project has adopted the ubiquitous `semantic-versioning' scheme, which is the standard system used for versioning in Julia \cite{JuliasProcess}. This details a three-number system of versioning specifying major versions, minor versions, and patches respectively. The expectation is that for a major version number of zero, the software is in an alpha state and any minor version update may introduce breaking changes, but patch changes should not do so. Major versions of one and greater are expected to be official releases, with an expectation of stability and correctness, and that further minor versions are unlikely to introduce breaking changes.

As an example of the timescales spent on each stage of this versioning system (although it of course varies between projects and their scales), JuMP version 0.1.0 was released publicly in October 2013, with steady releases of minor versions roughly every two months initially, with versions being less frequent and larger in scope as the project matured. In early 2020 it has reached version 0.20.1, and yet to announce its first major release. This is a common state in open-source software, and it is unlikely that any of the packages in this project will reach a state of maturity that allows them to take a version of 1.0.0 within the project timescales. However the advantage of having this as an open-source project is that work can continue after the initial project concludes.

As shown by JuMP, it is very possible to have pre-release software that is still high performance and stable. It is intended that this is the state that this project will reach.

\subsection{Testing}
Rigorous testing of code is crucial for any project that has an expectation of quality and correctness. There are two general forms of testing in use by this project; unit testing and continuous integration.

\subsubsection{Unit Testing}
A unit test is a series of small tests that are designed to ensure that a function gives the expected results for its main use case, as well as any possible edge cases. As unit tests should be quick to run, they are not able to check every edge case a package might encounter, with the intention being that they provide a `good enough' assurance of correctness. Ideally every function in software will have some unit-tests implemented, however it is often the case that many functions are only indirectly tested via the unit-tests of other functions.

 Julia contains a fairly rich unit-testing framework that can easily be run during development. Unit tests provide a certain expectation that the majority of the code is correct in most circumstances, assuming that the developer has taken the time to match their changes with updated tests. The code developed in this project currently has a minimal set of unit-tests, with the intention to increase their scope as the design becomes more stable.

\subsubsection{Continuous Integration}
Continuous integration is the act of ensuring that changes to code are frequently pushed back into the development repository, at which point automation software is able to run a larger set of tests on the codebase. Larger tests are able to take longer, running more edge-cases, benchmarks and providing more trust that the code is correct. These services are also able to ensure that the code is stable for running on multiple platforms, e.g. Linux, Windows, and macOS.

While an over-generalisation, it is useful to think of unit-tests as tests for syntax and function-level correctness, and continuous integration tests as testing the correctness of the overall software design. Continuous integration is also able to provide a detailed analysis of the code. For example, it can be set to run benchmarks showing that the performance of the code has not been decreased by new additions. It is also able to provide reports measuring code coverage, a metric showing that the tests are exercising a total percentage of the codebase. Ideally 100\% code coverage can be reached, showing that the entire codebase is being covered when tests are run. Unfortunately writing tests to find every edge case is very difficult, with coverage typically being in the range of 80\% to 95\%. 

\subsection{Version Control}
With the use of continuous integration, new code is potentially pushed to the main repository multiple times a day. As it would be irresponsible to update the available codebase with potentially broken additions, good use of version control software is necessary. The most common tool used by open-source software (and increasingly so in commercial projects) is Git \cite{Git}, generally through services like GitHub, GitLab, or BitBucket. 

Git allows for a single codebase to maintain multiple development paths running concurrently via branches. A branch can easily be created from any other, allowing a developer to work on a fix or feature without impacting any other code. Once complete, this code can be integrated back into its parent branch, ensuring that any new code is at least somewhat stable. The master branch is the branch that traditionally holds the current release version of the code. Merges of code from development branches back into the master branch is something that needs to be verified with other maintainers of the repository. Github (the git provider of choice for this project) provides a very powerful mechanism for inspecting changes, and how changes would affect the new release in terms of code coverage and any breaking changes. These can be discussed and decided, with controls put in place to ensure that only working code is released.
